@online{8CoordinateReference,
  title = {8. {{Coordinate Reference Systems}} — {{QGIS Documentation}} Documentation},
  url = {https://docs.qgis.org/3.40/en/docs/gentle_gis_introduction/coordinate_reference_systems.html},
  urldate = {2025-03-26},
  file = {/Users/kristjanluik/Zotero/storage/IIC2IAGY/coordinate_reference_systems.html}
}

@article{cheungPerimeterDefense42015,
  title = {Perimeter {{Defense}}: 4 {{Technologies}} for {{Detecting}} and {{Preventing Illegal Logging}}},
  shorttitle = {Perimeter {{Defense}}},
  author = {Cheung, Loretta and Mason, Jonathan and Parker-Forney, Meaghan},
  year = {Fri, 11/06/2015 - 11:16},
  url = {https://www.wri.org/insights/perimeter-defense-4-technologies-detecting-and-preventing-illegal-logging},
  urldate = {2025-03-01},
  abstract = {From drones to infrared sensors to crowdsourcing applications, forest defenders are increasingly turning to technology to stop illegal logging.},
  langid = {english},
  file = {/Users/kristjanluik/Zotero/storage/Q4SSVTPB/perimeter-defense-4-technologies-detecting-and-preventing-illegal-logging.html}
}

@online{CopernicusCopernicus,
  title = {About {{Copernicus}} | {{Copernicus}}},
  url = {https://www.copernicus.eu/en/about-copernicus},
  urldate = {2025-03-04},
  file = {/Users/kristjanluik/Zotero/storage/YNKVILCK/about-copernicus.html}
}

@online{DINOv2RocksGeological,
  title = {{{DINOv2 Rocks Geological Image Analysis}}: {{Classification}}, {{Segmentation}}, and {{Interpretability}}},
  url = {https://arxiv.org/html/2407.18100v3},
  urldate = {2025-05-07},
  file = {/Users/kristjanluik/Zotero/storage/CHVJFE8K/2407.html}
}

@online{ExtendingGloballocalView,
  title = {Extending Global-Local View Alignment for Self-Supervised Learning with Remote Sensing Imagery},
  url = {https://arxiv.org/html/2303.06670v2},
  urldate = {2025-05-07},
  file = {/Users/kristjanluik/Zotero/storage/9H9EK79N/2303.html}
}

@inproceedings{hwangSAM2AbdomenOneshot2024,
  title = {{{SAM2}} for Abdomen: {{One-shot}} and No Finetuning},
  shorttitle = {{{SAM2}} for Abdomen},
  booktitle = {2024 9th {{International Conference}} on {{Intelligent Informatics}} and {{Biomedical Sciences}} ({{ICIIBMS}})},
  author = {Hwang, Jongyun and Na, Inye and Kim, Jonghun and Park, Hyunjin},
  date = {2024-11},
  volume = {9},
  pages = {551--555},
  issn = {2189-8723},
  doi = {10.1109/ICIIBMS62405.2024.10792815},
  url = {https://ieeexplore.ieee.org/document/10792815/},
  urldate = {2025-05-16},
  abstract = {The study introduces SAM2, a novel method for fully automatic abdominal organ segmentation using the Segment Anything Model 2 (SAM2) and Self-Supervised Anatomical Embedding (Emb-SAM). Unlike traditional approaches, our model achieves superior segmentation performance without the need for finetuning or large labeled datasets, relying on a single labeled image. SAM2 leverages its memory bank for mask propagation, originally designed for video segmentation, which we adapt for 3D medical imaging by treating CT slices as sequential frames. Emb-SAM generates precise pseudo-labels by matching anatomical points across images using self-supervised learning. The proposed method effectively segments abdominal organs such as the liver, kidneys, spleen, and aorta, demonstrating superior consistency compared to baseline models on the BTCV dataset. Experimental results show that our model achieves competitive performance, significantly reducing computational costs and manual intervention, thus offering a promising solution for automated medical image segmentation.},
  eventtitle = {2024 9th {{International Conference}} on {{Intelligent Informatics}} and {{Biomedical Sciences}} ({{ICIIBMS}})},
  keywords = {Abdomen segmentation,Adaptation models,Biological system modeling,Biological systems,Biomedical imaging,Computational modeling,Image segmentation,Manuals,no finetuning,one-shot segmentation,segment anything model 2,Self-supervised learning,Spleen,Three-dimensional displays},
  file = {/Users/kristjanluik/Zotero/storage/X2K8PJZU/Hwang et al. - 2024 - SAM2 for abdomen One-shot and no finetuning.pdf}
}

@online{InfrastructureOverviewCopernicus,
  title = {Infrastructure {{Overview}} | {{Copernicus}}},
  url = {https://www.copernicus.eu/en/about-copernicus/infrastructure-overview},
  urldate = {2025-03-01},
  file = {/Users/kristjanluik/Zotero/storage/E4ZZFDAJ/infrastructure-overview.html}
}

@online{IntersectionUnionIoU,
  title = {Intersection over {{Union}} ({{IoU}}): {{Definition}}, {{Calculation}}, {{Code}}},
  shorttitle = {Intersection over {{Union}} ({{IoU}})},
  url = {https://www.v7labs.com/blog/intersection-over-union-guide},
  urldate = {2025-02-26},
  abstract = {Find out how and when to use IoU, one of the most crucial model model assessment metrics.},
  langid = {english},
  file = {/Users/kristjanluik/Zotero/storage/PJRWV7TF/intersection-over-union-guide.html}
}

@article{isaienkovDeepLearningRegular2021,
  title = {Deep {{Learning}} for {{Regular Change Detection}} in {{Ukrainian Forest Ecosystem}} with {{Sentinel-2}}},
  author = {Isaienkov, K. and Yushchuk, M. and Khramtsov, V. and Seliverstov, O.},
  date = {2021},
  journaltitle = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  volume = {14},
  pages = {364--376},
  doi = {10.1109/JSTARS.2020.3034186},
  abstract = {The logging is the leading cause for the reduction in the forest area in the world. At the same time, the number of forest clearcuts continues to grow. However, despite the massive scale, such incidents are difficult to track in time. As a result, huge areas of forests are gradually being cut down. Therefore, there is a need for regular and effective monitoring of changes in forest cover. The multitemporal data sources like Copernicus Sentinel-2 allow enhancing the potential of monitoring the Earth's surface and environmental dynamics including forest plantations. In this article, we present a baseline U-Net model for deforestation detection in the forest-steppe zone. Training and evaluation are conducted on our own dataset created on Sentinel-2 imagery for the Kharkiv region of Ukraine (31 400 km\textasciicircum 2). As a part of the research, we present several models with the ability to work with time-dependent imagery. The main contribution of this article is to provide a baseline model for the forest change detection inside Ukraine and improve it adding the ability to use several sequential images as an input of the segmentation model. © 2008-2012 IEEE.},
  keywords = {Change detection,convolutional neural network (CNN),deep learning,deforestation,logging,LSTM,optical imagery,semantic segmentation,U-Net},
  file = {/Users/kristjanluik/Zotero/storage/ZUQ3RTX2/display.html}
}

@inproceedings{karraGlobalLandUse2021,
  title = {Global Land Use / Land Cover with {{Sentinel}} 2 and Deep Learning},
  booktitle = {2021 {{IEEE International Geoscience}} and {{Remote Sensing Symposium IGARSS}}},
  author = {Karra, Krishna and Kontgis, Caitlin and Statman-Weil, Zoe and Mazzariello, Joseph C. and Mathis, Mark and Brumby, Steven P.},
  date = {2021-07},
  pages = {4704--4707},
  issn = {2153-7003},
  doi = {10.1109/IGARSS47720.2021.9553499},
  url = {https://ieeexplore.ieee.org/document/9553499/},
  urldate = {2025-04-18},
  abstract = {Land use/land cover (LULC) maps are foundational geospatial data products needed by analysts and decision makers across governments, civil society, industry, and finance to monitor global environmental change and measure risk to sustainable livelihoods and development. There is a strong need for high-level, automated geospatial analysis products that turn these pixels into actionable insights for non-geospatial experts. The Sentinel 2 satellites, first launched in mid-2015, are excellent candidates for LULC mapping due to their high spatial, spectral, and temporal resolution. Advances in deep learning and scalable cloud-based compute now provide the analysis capability required to unlock the value in global satellite imagery observations. Based on a novel, very large dataset of over 5 billion human-labeled Sentinel-2 pixels, we developed and deployed a deep learning segmentation model on Sentinel-2 data to create a global LULC map at 10m resolution that achieves state-of-the-art accuracy and enables automated LULC mapping from time series observations.},
  eventtitle = {2021 {{IEEE International Geoscience}} and {{Remote Sensing Symposium IGARSS}}},
  keywords = {deep learning,Deep learning,Geoscience and remote sensing,Government,Image segmentation,Industries,land use land cover,Satellites,segmentation,Sentinel 2,Time series analysis},
  file = {/Users/kristjanluik/Zotero/storage/2ZRYMARK/Karra et al. - 2021 - Global land use  land cover with Sentinel 2 and deep learning.pdf}
}

@article{karsentyUnderlyingCausesRapid2003,
  title = {Underlying Causes of the Rapid Expansion of Illegal Exploitation of Tropical Timber},
  author = {Karsenty, A.},
  date = {2003-09-01},
  journaltitle = {International Forestry Review},
  shortjournal = {int. forest. rev.},
  volume = {5},
  number = {3},
  pages = {236--239},
  issn = {1465-5489},
  doi = {10.1505/IFOR.5.3.236.19136},
  url = {http://www.ingentaconnect.com/content/10.1505/IFOR.5.3.236.19136},
  urldate = {2025-03-04},
  langid = {english}
}

@online{kirillovSegmentAnything2023,
  title = {Segment {{Anything}}},
  author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
  date = {2023-04-05},
  eprint = {2304.02643},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.02643},
  url = {http://arxiv.org/abs/2304.02643},
  urldate = {2025-05-17},
  abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/kristjanluik/Zotero/storage/T6LHC4UJ/Kirillov et al. - 2023 - Segment Anything.pdf;/Users/kristjanluik/Zotero/storage/MQQ4M2Q3/2304.html}
}

@article{kroupiDeepConvolutionalNeural2019,
  title = {Deep Convolutional Neural Networks for Land-Cover Classification with {{Sentinel-2}} Images},
  author = {Kroupi, Eleni and Kesa, Maria and Navarro-Sánchez, Victor Diego and Saeed, Salman and Pelloquin, Camille and Alhaddad, Bahaa and Moreno, Laura and Soria-Frisch, Aureli and Ruffini, Giulio},
  date = {2019-06},
  journaltitle = {Journal of Applied Remote Sensing},
  shortjournal = {JARS},
  volume = {13},
  number = {2},
  pages = {024525},
  publisher = {SPIE},
  issn = {1931-3195, 1931-3195},
  doi = {10.1117/1.JRS.13.024525},
  url = {https://www.spiedigitallibrary.org/journals/journal-of-applied-remote-sensing/volume-13/issue-2/024525/Deep-convolutional-neural-networks-for-land-cover-classification-with-Sentinel/10.1117/1.JRS.13.024525.full},
  urldate = {2025-04-18},
  abstract = {Currently, analyzing satellite images requires an unsustainable amount of manual labor. Semiautomatic solutions for land-cover classification of satellite images entail the incorporation of expert knowledge. To increase the scalability of the built solutions, methods that automate image processing and analysis pipelines are required. Recently, deep learning (DL) models have been applied to challenging vision problems with great success. We expect that the use of DL models will soon outperform shallow networks and other classification algorithms, as recently achieved in multiple domains. Here, we consider the task of land-cover classification of satellite images. This seems particularly appropriate for deep classifiers due to the combined high dimensionality of the data with the presence of compositional dependencies between pixels, which can be used to characterize a particular class. We develop a pipeline for analyzing satellite images using a deep convolutional neural network for practical applications. We present its successful application for land-cover classification, where it achieves 86\% classification accuracy on unseen raw images.}
}

@online{kunduAssessingPerformanceDINOv22024,
  title = {Assessing the {{Performance}} of the {{DINOv2 Self-supervised Learning Vision Transformer Model}} for the {{Segmentation}} of the {{Left Atrium}} from {{MRI Images}}},
  author = {Kundu, Bipasha and Khanal, Bidur and Simon, Richard and Linte, Cristian A.},
  date = {2024-11-14},
  eprint = {2411.09598},
  eprinttype = {arXiv},
  eprintclass = {eess},
  doi = {10.48550/arXiv.2411.09598},
  url = {http://arxiv.org/abs/2411.09598},
  urldate = {2025-05-07},
  abstract = {Accurate left atrium (LA) segmentation from pre-operative scans is crucial for diagnosing atrial fibrillation, treatment planning, and supporting surgical interventions. While deep learning models are key in medical image segmentation, they often require extensive manually annotated data. Foundation models trained on larger datasets have reduced this dependency, enhancing generalizability and robustness through transfer learning. We explore DINOv2, a self-supervised learning vision transformer trained on natural images, for LA segmentation using MRI. The challenges for LA's complex anatomy, thin boundaries, and limited annotated data make accurate segmentation difficult before \& during the image-guided intervention. We demonstrate DINOv2's ability to provide accurate \& consistent segmentation, achieving a mean Dice score of .871 \& a Jaccard Index of .792 for end-to-end fine-tuning. Through few-shot learning across various data sizes \& patient counts, DINOv2 consistently outperforms baseline models. These results suggest that DINOv2 effectively adapts to MRI with limited data, highlighting its potential as a competitive tool for segmentation \& encouraging broader use in medical imaging.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/kristjanluik/Zotero/storage/7IYG3PNX/Kundu et al. - 2024 - Assessing the Performance of the DINOv2 Self-supervised Learning Vision Transformer Model for the Se.pdf;/Users/kristjanluik/Zotero/storage/3I768AC9/2411.html}
}

@online{maa-ametNationalSatelliteData,
  title = {National {{Satellite Data Centre ESTHub}}},
  author = {Maa-amet},
  url = {https://geoportaal.maaamet.ee/eng/spatial-data/national-satellite-data-centre-esthub-p654.html},
  urldate = {2025-03-10},
  abstract = {Geoportal - Webpage of Republic of Estonia Land and Spatial Development Board: Web Maps, Spatial Data, Maps, Services, Open Data},
  langid = {english},
  file = {/Users/kristjanluik/Zotero/storage/3ZBPURM9/national-satellite-data-centre-esthub-p654.html}
}

@online{maa-ametRiiklikSatelliidiandmeteKeskus,
  title = {Riiklik satelliidiandmete keskus ESTHub},
  author = {Maa-amet},
  url = {https://geoportaal.maaamet.ee/est/ruumiandmed/riiklik-satelliidiandmete-keskus-esthub-p443.html},
  urldate = {2025-02-26},
  abstract = {Geoportaal - Maa ja Ruumiameti ruumiandmete veebileht: kaardirakendused, ruumiandmed, kaardid, teenused, andmete allalaadimine},
  langid = {estonian},
  file = {/Users/kristjanluik/Zotero/storage/KRTXEHN4/riiklik-satelliidiandmete-keskus-esthub-p443.html}
}

@online{MetsaKorraldamiseJuhend,
  title = {Metsa Korraldamise Juhend–{{Riigi Teataja}}},
  url = {https://www.riigiteataja.ee/akt/13124148?leiaKehtiv},
  urldate = {2025-04-20},
  file = {/Users/kristjanluik/Zotero/storage/UDKNUMKB/13124148.html}
}

@online{MetsateatisJaMetsaregister,
  title = {Metsateatis ja metsaregister | Keskkonnaamet},
  url = {https://keskkonnaamet.ee/elusloodus-looduskaitse/metsandus/metsateatis-ja-metsaregister},
  urldate = {2025-04-02},
  abstract = {Metsateatis on dokument, mille metsaomanik esitab Keskkonnaametile kavandatavate raietööde või oluliste metsakahjustuste kohta. Keskkonnaamet kontrollib metsateatise nõuetekohasust ja kavandatud raie vastavust õigusaktidega kehtestatud nõuetele.},
  langid = {estonian},
  file = {/Users/kristjanluik/Zotero/storage/LQA92HVS/metsateatis-ja-metsaregister.html}
}

@online{oquabDINOv2LearningRobust2024,
  title = {{{DINOv2}}: {{Learning Robust Visual Features}} without {{Supervision}}},
  shorttitle = {{{DINOv2}}},
  author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
  date = {2024-02-02},
  eprint = {2304.07193},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.07193},
  url = {http://arxiv.org/abs/2304.07193},
  urldate = {2025-04-19},
  abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/kristjanluik/Zotero/storage/NPXNLV9S/Oquab et al. - 2024 - DINOv2 Learning Robust Visual Features without Supervision.pdf;/Users/kristjanluik/Zotero/storage/FIM9L63U/2304.html}
}

@online{PanopticonAdvancingAnySensor,
  title = {Panopticon: {{Advancing Any-Sensor Foundation Models}} for {{Earth Observation}}},
  url = {https://arxiv.org/html/2503.10845v1},
  urldate = {2025-05-07},
  file = {/Users/kristjanluik/Zotero/storage/E43VJEVN/2503.html}
}

@article{pearseDevelopingForestDescription2025,
  title = {Developing a Forest Description from Remote Sensing: {{Insights}} from {{New Zealand}}},
  shorttitle = {Developing a Forest Description from Remote Sensing},
  author = {Pearse, Grant D. and Jayathunga, Sadeepa and Camarretta, Nicolò and Palmer, Melanie E. and Steer, Benjamin S. C. and Watt, Michael S. and Watt, Pete and Holdaway, Andrew},
  date = {2025-06-01},
  journaltitle = {Science of Remote Sensing},
  shortjournal = {Science of Remote Sensing},
  volume = {11},
  pages = {100183},
  issn = {2666-0172},
  doi = {10.1016/j.srs.2024.100183},
  url = {https://www.sciencedirect.com/science/article/pii/S2666017224000671},
  urldate = {2025-04-06},
  abstract = {Remote sensing is increasingly being used to create large-scale forest descriptions. In New Zealand, where radiata pine (Pinus radiata) plantations dominate the forestry sector, the current national forest description lacks spatially explicit information and struggles to capture data on small-scale forests. This is important as these forests are expected to contribute significantly to future wood supply and carbon sequestration. This study demonstrates the development of a spatially explicit, remote sensing-based forest description for the Gisborne region, a major forest growing area. We combined deep learning-based forest mapping using high-resolution aerial imagery with regional airborne laser scanning (ALS) data to map all planted forest and estimate key attributes. The deep learning model accurately delineated planted forests, including large estates, small woodlots, and newly established stands as young as 3-years post planting. It achieved an intersection over union of 0.94, precision of 0.96, and recall of 0.98 on a withheld dataset. ALS-derived models for estimating mean top height, total stem volume, and stand age showed good performance (R2~=~0.94, 0.82, and 0.94 respectively). The resulting spatially explicit forest description provides wall-to-wall information on forest extent, age, and volume for all sizes of forest. This enables stratification by key variables for wood supply forecasting, harvest planning, and infrastructure investment decisions. We propose satellite-based harvest detection and digital photogrammetry to continuously update the initial forest description. This methodology enables near real-time monitoring of planted forests at all scales and is adaptable to other regions with similar data availability.},
  keywords = {Aerial imagery,Airborne laser scanning,Deep learning,Forest inventory,Forestry,Lidar,Radiata pine},
  file = {/Users/kristjanluik/Zotero/storage/MAU7Y6KT/S2666017224000671.html}
}

@inproceedings{podoprigorovaRecognitionForestDamage2024,
  title = {Recognition of {{Forest Damage}} from {{Sentinel-2 Satellite Images Using U-Net}}, {{RandomForest}} and {{XGBoost}}},
  author = {Podoprigorova, N.S. and Safonov, F.A. and Podoprigorova, S.S. and Tarasov, A.V. and Shikhov, A.N.},
  date = {2024},
  doi = {10.1109/REEPE60449.2024.10479810},
  abstract = {Automatic analysis of satellite images is an actual problem today. This paper compares methods for semantic segmentation of forest damage in different regions of the European territory of Russia and the Urals based on multi-temporal Sentinel-2 satellite imagery. In this paper, it is selected the most informative features among the different spectral bands, their differences and the vegetation indices based on them. Three segmentation methods are compared: random forest, gradient boosting and U-net architecture convolutional neural network. The Dice coefficient is used for evaluation. As a result, the U-Net CNN model was the highest with a coefficient of 78.69\% compared to 55.85\% and 54.06\% respectively for the best RF and XGBoost models. © 2024 IEEE.},
  eventtitle = {Proceedings of the 2024 6th {{International Youth Conference}} on {{Radio Electronics}}, {{Electrical}} and {{Power Engineering}}, {{REEPE}} 2024},
  keywords = {convolutional neural networks,forest damage,land cover,Random Forest,satellite imagery,semantic segmentation,Sentinel-2 data,U-Net,XGBoost},
  file = {/Users/kristjanluik/Zotero/storage/4PMKPBYI/display.html}
}

@inproceedings{prittSatelliteImageClassification2017,
  title = {Satellite {{Image Classification}} with {{Deep Learning}}},
  booktitle = {2017 {{IEEE Applied Imagery Pattern Recognition Workshop}} ({{AIPR}})},
  author = {Pritt, Mark and Chern, Gary},
  date = {2017-10},
  pages = {1--7},
  issn = {2332-5615},
  doi = {10.1109/AIPR.2017.8457969},
  url = {https://ieeexplore.ieee.org/abstract/document/8457969?casa_token=pxcT2lmIuZQAAAAA:cxTvI-np3CgbsWdBNiOVEzcrbFEDnPdVqlDbK2SephTl7gezFIqQ-D75ShKZiwsMIFSvdzD1I10},
  urldate = {2025-01-15},
  abstract = {Satellite imagery is important for many applications including disaster response, law enforcement, and environmental monitoring. These applications require the manual identification of objects and facilities in the imagery. Because the geographic expanses to be covered are great and the analysts available to conduct the searches are few, automation is required. Yet traditional object detection and classification algorithms are too inaccurate and unreliable to solve the problem. Deep learning is a family of machine learning algorithms that have shown promise for the automation of such tasks. It has achieved success in image understanding by means of convolutional neural networks. In this paper we apply them to the problem of object and facility recognition in high-resolution, multi-spectral satellite imagery. We describe a deep learning system for classifying objects and facilities from the IARPA Functional Map of the World (fMoW) dataset into 63 different classes. The system consists of an ensemble of convolutional neural networks and additional neural networks that integrate satellite metadata with image features. It is implemented in Python using the Keras and TensorFlow deep learning libraries and runs on a Linux server with an NVIDIA Titan X graphics card. At the time of writing the system is in 2nd place in the fMoW TopCoder competition. Its total accuracy is 83\%, the F1score is 0.797, and it classifies 15 of the classes with accuracies of 95\% or better.},
  eventtitle = {2017 {{IEEE Applied Imagery Pattern Recognition Workshop}} ({{AIPR}})},
  keywords = {AI,artificial intelligence,classification,Classification algorithms,deep learning,Detectors,Feature extraction,image understanding,machine learning,Machine learning,Metadata,recognition,satellite imagery,Satellites,Training},
  file = {/Users/kristjanluik/Zotero/storage/5LKY8JCG/Pritt and Chern - 2017 - Satellite Image Classification with Deep Learning.pdf;/Users/kristjanluik/Zotero/storage/95WUZYT2/8457969.html}
}

@online{raviSAM2Segment2024,
  title = {{{SAM}} 2: {{Segment Anything}} in {{Images}} and {{Videos}}},
  shorttitle = {{{SAM}} 2},
  author = {Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and Rädle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Dollár, Piotr and Feichtenhofer, Christoph},
  date = {2024-10-28},
  eprint = {2408.00714},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.00714},
  url = {http://arxiv.org/abs/2408.00714},
  urldate = {2025-05-16},
  abstract = {We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/kristjanluik/Zotero/storage/22JR352Q/Ravi et al. - 2024 - SAM 2 Segment Anything in Images and Videos.pdf;/Users/kristjanluik/Zotero/storage/Y93WXMU8/2408.html}
}

@online{rongMPGSAM2Adapting2025,
  title = {{{MPG-SAM}} 2: {{Adapting SAM}} 2 with {{Mask Priors}} and {{Global Context}} for {{Referring Video Object Segmentation}}},
  shorttitle = {{{MPG-SAM}} 2},
  author = {Rong, Fu and Lan, Meng and Zhang, Qian and Zhang, Lefei},
  date = {2025-03-10},
  eprint = {2501.13667},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2501.13667},
  url = {http://arxiv.org/abs/2501.13667},
  urldate = {2025-05-16},
  abstract = {Referring video object segmentation (RVOS) aims to segment objects in a video according to textual descriptions, which requires the integration of multimodal information and temporal dynamics perception. The Segment Anything Model 2 (SAM 2) has shown great effectiveness across various video segmentation tasks. However, its application to offline RVOS is challenged by the translation of the text into effective prompts and a lack of global context awareness. In this paper, we propose a novel RVOS framework, termed MPG-SAM 2, to address these challenges. Specifically, MPG-SAM 2 employs a unified multimodal encoder to jointly encode video and textual features, generating semantically aligned video and text embeddings, along with multimodal class tokens. A mask prior generator utilizes the video embeddings and class tokens to create pseudo masks of target objects and global context. These masks are fed into the prompt encoder as dense prompts along with multimodal class tokens as sparse prompts to generate accurate prompts for SAM 2. To provide the online SAM 2 with a global view, we introduce a hierarchical global-historical aggregator, which allows SAM 2 to aggregate global and historical information of target objects at both pixel and object levels, enhancing the target representation and temporal consistency. Extensive experiments on several RVOS benchmarks demonstrate the superiority of MPG-SAM 2 and the effectiveness of our proposed modules.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/kristjanluik/Zotero/storage/K47BGDRN/Rong et al. - 2025 - MPG-SAM 2 Adapting SAM 2 with Mask Priors and Global Context for Referring Video Object Segmentatio.pdf}
}

@online{S1Applications,
  title = {S1 {{Applications}}},
  url = {https://sentiwiki.copernicus.eu/web/s1-applications},
  urldate = {2025-03-04},
  abstract = {Overview of S1 Applications Sentinel-1 delivers radar imagery for numerous applications. SAR images are the best way of tracking land subsidence an...},
  langid = {english}
}

@online{S2Applications,
  title = {S2 {{Applications}}},
  url = {https://sentiwiki.copernicus.eu/web/s2-applications},
  urldate = {2025-03-04},
  abstract = {Overview of S2 Applications The Sentinel-2 mission requirements is to ensure a high revisit frequency and high resolution imagery to support Copern...},
  langid = {english},
  file = {/Users/kristjanluik/Zotero/storage/RBWVXK8H/s2-applications.html}
}

@online{S2Mission,
  title = {S2 {{Mission}}},
  url = {https://sentiwiki.copernicus.eu/web/s2-mission},
  urldate = {2025-03-25},
  abstract = {Overview of Sentinel-2 Mission Sentinel-2 is a European wide-swath, high-resolution, multi-spectral imaging mission. The full mission specification...},
  langid = {english},
  file = {/Users/kristjanluik/Zotero/storage/9BZN8I8P/s2-mission.html}
}

@online{S3Mission,
  title = {S3 {{Mission}}},
  url = {https://sentiwiki.copernicus.eu/web/s3-mission},
  urldate = {2025-03-04},
  abstract = {Overview of Sentinel-3 Mission Sentinel-3 is an European Earth Observation satellite mission developed to support Copernicus ocean, land, atmospher...},
  langid = {english},
  file = {/Users/kristjanluik/Zotero/storage/FQ5U8H2I/s3-mission.html}
}

@online{S5PApplications,
  title = {{{S5P Applications}}},
  url = {https://sentiwiki.copernicus.eu/web/s5p-applications},
  urldate = {2025-03-04},
  abstract = {Overview of Sentinel- 5P Applications The three environmental themes covered by TROPOMI are Air Quality, Stratospheric Ozone Layer and Climate Chan...},
  langid = {english},
  file = {/Users/kristjanluik/Zotero/storage/N6BLMMQZ/s5p-applications.html}
}

@online{Sentinel2OverviewScienceDirect,
  title = {Sentinel-2 - an Overview | {{ScienceDirect Topics}}},
  url = {https://www.sciencedirect.com/topics/earth-and-planetary-sciences/sentinel-2},
  urldate = {2025-03-01},
  file = {/Users/kristjanluik/Zotero/storage/E9PB3HXF/sentinel-2.html}
}

@online{TartuUlikooliTeadlased2020,
  title = {Tartu Ülikooli teadlased töötasid välja statistilised meetodid Eesti metsaressursi hindamiseks kaugseireandmete põhjal | Tartu Ülikool},
  year = {R, 11.09.2020 - 11:05},
  url = {https://ut.ee/et/sisu/tartu-ulikooli-teadlased-tootasid-valja-statistilised-meetodid-eesti-metsaressursi-hindamiseks},
  urldate = {2025-01-11},
  abstract = {Sel suvel lõppes Tartu Ülikooli matemaatika ja statistika instituudi ning Tõravere observatooriumi teadlaste pea kaks aastat kestnud uuring, mille eesmärk oli arendada statistilisi meetodei},
  langid = {estonian},
  file = {/Users/kristjanluik/Zotero/storage/FLRMXUYA/tartu-ulikooli-teadlased-tootasid-valja-statistilised-meetodid-eesti-metsaressursi-hindamiseks.html}
}

@online{UnderstandingDICECOEFFICIENT,
  title = {Understanding {{DICE COEFFICIENT}}},
  url = {https://kaggle.com/code/yerramvarun/understanding-dice-coefficient},
  urldate = {2025-02-26},
  abstract = {Explore and run machine learning code with Kaggle Notebooks | Using data from HuBMAP - Hacking the Kidney},
  langid = {english},
  file = {/Users/kristjanluik/Zotero/storage/BMJ8V6L4/understanding-dice-coefficient.html}
}

@online{WhatAreFoundation,
  title = {What Are {{Foundation Models}}? - {{Foundation Models}} in {{Generative AI Explained}} - {{AWS}}},
  shorttitle = {What Are {{Foundation Models}}?},
  url = {https://aws.amazon.com/what-is/foundation-models/},
  urldate = {2025-04-19},
  abstract = {Learn why Foundation Models are essential. Discover its benefits and how you can use it to create new content and ideas including text, conversations, images, video, and audio with Generative AI.},
  langid = {american},
  organization = {Amazon Web Services, Inc.},
  file = {/Users/kristjanluik/Zotero/storage/EXFY44R2/foundation-models.html}
}

@online{WhatLocationPolygon,
  title = {What Is a Location Polygon?},
  url = {https://www.narrative.io/knowledge-base/concepts/data-terms/what-is-a-location-polygon},
  urldate = {2025-04-02},
  abstract = {A location polygon is a shape that represents a point of interest using geospatial coordinates.},
  langid = {english},
  file = {/Users/kristjanluik/Zotero/storage/MB8PKV4G/what-is-a-location-polygon.html}
}

@online{WWFImportanceForests,
  title = {{{WWF}} - {{The Importance}} of {{Forests}}},
  url = {https://wwf.panda.org/discover/our_focus/forests_practice/importance_forests/},
  urldate = {2025-03-04},
  abstract = {Forests impact on our daily lives, even in the midst of a busy, noisy, concrete city centre. Despite our dependence on forests, we are still allowing them to disappear. Act now with WWF},
  langid = {english},
  file = {/Users/kristjanluik/Zotero/storage/WP5PI496/importance_forests.html}
}
